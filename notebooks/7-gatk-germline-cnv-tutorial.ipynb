{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "US-9OQ8XgZQF"
   },
   "source": [
    "\n",
    "# GATK Germline Copy Number Variation Tutorial <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**March 2023**  \n",
    "\n",
    "<img src=\"https://storage.googleapis.com/broad-dsde-methods-gatk-workshop-public/images/germline-cnv-tutorial/gatk-gcnv-pipeline-overview.png\" alt=\"drawing\" width=\"100%\" align=\"center\"/>\n",
    "\n",
    "\n",
    "The tutorial outlines steps in detecting germline copy number variants using GATK-gCNVs and illustrates two workflow modes: **cohort mode** and the **case mode**. The cohort mode simultaneously generates a cohort model and calls CNVs for the cohort samples. The case mode analyzes a single sample against an already constructed cohort model. The same workflow steps apply to both targeted exome and whole genome sequencing (WGS) data. The workflow is able to call both rare and common events and intelligently handles allosomal ploidies, i.e. cohorts of mixed male and female samples.\n",
    "\n",
    "For the cohort mode, the general recommendation is at least a hundred samples to start. Researchers should expect to tune workflow parameters from the provided defaults. In particular, GermlineCNVCaller's default inference parameters are conservatively set for efficient run times.\n",
    "\n",
    "The figure above diagrams the workflow tools. **Section 1** sets up the Notebook environment and downloads the data and resource files needed to complete this tutorial. **Section 2** creates an intervals list and counts read alignments overlapping the intervals. **Section 3** shows optional but recommended cohort mode steps to annotate intervals with covariates for use in filtering intervals as well as for use in explicit modeling. The section also removes outlier counts intervals. **Section 4** generates global baseline observations for the data and models and calls the ploidy of each contig (ploidy means the overall baseline copy number of the contig). **Section 5** is at the heart of the workflow and models per-interval copy number. Because of the high intensity of compute model fitting requires, the section shows how to analyze data in parts. Finally, **Section 6** calls per-sample copy number events per interval and per segment. Results are in VCF format.\n",
    "\n",
    "➤ This guided Notebook demo is largely based on [Tutorial #11682](https://gatk.broadinstitute.org/hc/en-us/articles/360035531152--How-to-Call-common-and-rare-germline-copy-number-variants)   \n",
    "➤ A highly recommended [paper](https://www.nature.com/articles/s41588-023-01449-0) detailing the methods was published in _Nature Genetics_ in 2023.  \n",
    "➤ For pipelined workflows, see the WDL scripts in this Terra workspace.  \n",
    "➤ This workflow is **not** appropriate for bulk tumor samples, as it infers absolute copy numbers. For somatic copy number alteration calling, see [Tutorial #11682](https://gatk.zendesk.com/hc/en-us/articles/360035531092).\n",
    "\n",
    "_This tutorial was last tested with the GATK v4.2.4.0 and IGV v2.8.0._\n",
    " See [GATK Tool Documentation](https://gatk.broadinstitute.org/hc/en-us/articles/360037224712) for further information on the tools we use below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgzJFRYkgZQH"
   },
   "source": [
    "# Set up your Notebook\n",
    "\n",
    "\n",
    "## Set cloud environment values\n",
    "If you are opening a notebook for the first time today and you didn't adjust any runtime values, now's the time to edit them. Click on the gear icon in the upper right to edit your Notebook Runtime. Set the values as specified below:\n",
    "\n",
    "| Option | Value |\n",
    "| ------ | ------ |\n",
    "| Environment | [Custom](https://github.com/broadinstitute/gatk-workshop-terra-jupyter-image/wiki/Using-the-gatk%E2%80%90workshop%E2%80%90terra%E2%80%90jupyter%E2%80%90image-in-the-Terra-Jupyter-environment#6-type-the-command-setup_gatk_env-in-the-terminal-and-hit-enter) |\n",
    "| Profile | Custom |\n",
    "| CPU | 4 |\n",
    "| Disk size | 100 GB |\n",
    "| Memory | 15 GB |\n",
    "\n",
    "**Please Note:** This notebook currently requires the use of a custom environment, as described [here](https://github.com/broadinstitute/gatk-workshop-terra-jupyter-image/wiki/Using-the-gatk%E2%80%90workshop%E2%80%90terra%E2%80%90jupyter%E2%80%90image-in-the-Terra-Jupyter-environment#6-type-the-command-setup_gatk_env-in-the-terminal-and-hit-enter).\n",
    "\n",
    "Click the \"create\"/\"Update\" button when you are done, and Terra will begin to create a new runtime with your settings. When it is finished, it will pop up asking you to apply the new settings. In the meantime, you can continue with the setup instructions below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the GATK conda environment\n",
    "\n",
    "Run the following commands in the Terra terminal (available from the right panel):\n",
    "\n",
    "```\n",
    "setup_gatk_env\n",
    "```\n",
    "\n",
    "When you are done, you should close this notebook and reopen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check kernel type selected in the Jupyter notebook\n",
    "For this particular notebook, we will be using a specialized kernel that we've added to our **Kernel** menu by following the previous two steps for creating the custom envrionemnt. If you've successfully done this, you should be able to select a kernel called `Python [conda env:gatkconda]`under the **Kernel** > **Change Kernel** menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your files\n",
    "Your notebook has a temporary folder that exists so long as your cluster is running. To see what files are in your notebook environment at any time, you can click on the Jupyter logo in the upper left corner. \n",
    "\n",
    "For this tutorial, we need to copy some files from this temporary folder to and from our workspace bucket. Run the two commands below to set up the workspace bucket variable and the file paths inside your notebook.\n",
    "\n",
    "<font color = \"green\"> **Tool Tip:** To run a cell in a notebook, press `SHIFT + ENTER`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZc1n9fxgZQI"
   },
   "outputs": [],
   "source": [
    "# Set your workspace bucket variable for this notebook.\n",
    "import os\n",
    "\n",
    "WORKSPACE_BUCKET = os.environ['WORKSPACE_BUCKET']\n",
    "WORKSHOP_BUCKET = 'gs://broad-dsde-methods-gatk-workshop-public'\n",
    "REFERENCE_BUCKET = 'gs://genomics-public-data/resources/broad/hg38/v0'\n",
    "WORKSPACE_LOCAL = '/home/jupyter/notebooks/germline-cnv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2vQ99YsgZQL"
   },
   "outputs": [],
   "source": [
    "# Create sandbox and file directories\n",
    "! mkdir -p $WORKSPACE_LOCAL/sandbox/\n",
    "! mkdir -p $WORKSPACE_LOCAL/ref/\n",
    "! mkdir -p $WORKSPACE_LOCAL/data/\n",
    "\n",
    "# Removes any old symbolic linked sandbox directory and adds a new link, in case you've run this before\n",
    "! rm -rf sandbox\n",
    "! ln -s $WORKSPACE_LOCAL/sandbox sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lG2m2totgZQN"
   },
   "source": [
    "## Check data permissions\n",
    "For this tutorial, we have hosted the starting files in a public Google bucket. We will first check that the data is available to your user account, and if it is not, we simply need to install Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pH5Yhgh6gZQO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if data is accessible. The command should list several gs:// URLs.\n",
    "! gsutil ls $WORKSHOP_BUCKET/germline-cnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not see gs:// URLs listed above, run this cell to install Google Cloud Storage. \n",
    "# Afterwards, restart the kernel with Kernel > Restart.\n",
    "#! pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_QGHTOCvgZQS"
   },
   "source": [
    "## Download data to local disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial provides example small WGS data sourced from the _1000 Genomes Project_. Cohort mode illustrations use 7 samples, while case mode illustrations analyze one sample against a cohort model made from the remaining 6 samples. The tutorial uses a fraction of the workflow's recommended hundred samples for ease of illustration. Furthermore, commands in each step use one of three differently sized intervals lists for efficiency. Coverage data are from the entirety of chr20, chrX and chrY. So although a step may analyze a subset of regions, it is possible to instead analyze all three contigs in case or cohort modes.\n",
    "\n",
    "Download tutorial_11684.tar.gz either from the Workshop Google bucket. The bundle includes data for [Notebook #11685](https://gatk.zendesk.com/hc/en-us/articles/360035890031) and [Notebook #11686](https://gatk.zendesk.com/hc/en-us/articles/360035889891). The tutorial also requires the GRCh38 reference FASTA, dictionary and index. These are available from the GATK Resource Bundle. The example data is from the [1000 Genomes project](http://www.internationalgenome.org/) Phase 3 aligned to GRCh38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TkCGW77gZQS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! gsutil cp $WORKSHOP_BUCKET/germline-cnv/tutorial_11684.tar.gz /home/jupyter/notebooks/germline-cnv/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvf $WORKSPACE_LOCAL/data/tutorial_11684.tar.gz -C $WORKSPACE_LOCAL/data > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a shortcut variable\n",
    "TUTORIAL_DATA_PATH = '/home/jupyter/notebooks/germline-cnv/data/Tutorial11684'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cs1hPg-IgZQX"
   },
   "source": [
    "---\n",
    "\n",
    "# Collect raw counts data with _PreprocessIntervals_ and _CollectReadCounts_\n",
    "PreprocessIntervals pads exome targets and bins WGS intervals. Binning refers to creating equally sized intervals across the reference. For example, 1000 base binning would define chr1:1-1000 as the first bin. Because counts of reads on reference N bases are not meaningful, the tool automatically excludes bins with all Ns. For GRCh38 chr1, non-N sequences start at base 10,001, so the first few bin become:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SId3hYJgZQY"
   },
   "source": [
    "## For WGS data, bin entirety of reference, e.g. with 1000 base intervals.\n",
    "(We skip this step in this tutorial, but the command-line is shown below for your reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jP5klLTAgZQZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  ! gatk PreprocessIntervals \\\n",
    "#         -R $REFERENCE_BUCKET/Homo_sapiens_assembly38.fasta \\\n",
    "#         --padding 0 \\\n",
    "#         -imr OVERLAPPING_ONLY \\\n",
    "#         -O $WORKSPACE_LOCAL/sandbox/grch38.preprocessed.interval_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a Picard-style intervals list of 1000 base bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For exome data, pad target regions, e.g. with 250 bases.\n",
    "(We skip this step in this tutorial since the data is WGS, but the command-line is shown below for your reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gatk PreprocessIntervals \\\n",
    "#         -R $REFERENCE_BUCKET/Homo_sapiens_assembly38.fasta \\\n",
    "#         -L $TUTORIAL_DATA_PATH/targets.interval_list \\\n",
    "#         --bin-length 0 \\\n",
    "#         -imr OVERLAPPING_ONLY \\\n",
    "#         -O $WORKSPACE_LOCAL/sandbox/targets.preprocessed.interval_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the tutorial, we only bin two contigs\n",
    "Lets subset the reference to just chr20 and chrY. Coverage data exists for chr20, chrX and chrY in this tutorial, so changing the following arguments to look at all three contigs is left as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk PreprocessIntervals \\\n",
    "        -R $REFERENCE_BUCKET/Homo_sapiens_assembly38.fasta \\\n",
    "        --bin-length 1000 \\\n",
    "        --padding 0 \\\n",
    "        -L chr20 -L chrY \\\n",
    "        -imr OVERLAPPING_ONLY \\\n",
    "        -O $WORKSPACE_LOCAL/sandbox/chr20Y.interval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us take a look\n",
    "! grep -v '^@' $WORKSPACE_LOCAL/sandbox/chr20Y.interval_list | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on select parameters of _PreprocessIntervals_\n",
    "\n",
    "- For WGS, the default 1000 `--bin-length` is the recommended starting point for typical 30x data. Be sure to set `--padding 0` to disable padding outside of given genomic regions. Bin size should correlate with depth of coverage, e.g. lower coverage data should use larger bin size while higher coverage data can support smaller bin size. The size of the bin defines the resolution of CNV calls. The factors to consider in sizing include how noisy the data is, average coverage depth and how even coverage is across the reference.\n",
    "\n",
    "- For targeted exomes, provide the exome capture kit's target intervals with `-L`, set `--bin-length 0` to disable binning and pad the intervals with `--padding 250` or other desired length.\n",
    "\n",
    "- Provide intervals to exclude from analysis with `--exclude-intervals` or `-XL`, e.g. centromeric regions. Consider using this option especially if data is aligned to a reference other than GRCh38. The workflow enables excluding regions later again using `-XL`. A frugal strategy is to collect read counts using the entirety of intervals and then to exclude undesirable regions later at the _FilterIntervals_ step (section 3), the _DetermineGermlineContigPloidy_ step (section 4), at the _GermlineCNVCaller_ step (section 6) and/or post-calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count reads per bin using _CollectReadCounts_\n",
    "_CollectReadCounts_ tabulates the raw integer counts of reads overlapping an interval. The tutorial has already collected read counts ahead of time for the three contigs: chr20 and chrY. Here, we collect read counts on small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk CollectReadCounts \\\n",
    "        -L $TUTORIAL_DATA_PATH/chr20sub.interval_list \\\n",
    "        -R $REFERENCE_BUCKET/Homo_sapiens_assembly38.fasta \\\n",
    "        -imr OVERLAPPING_ONLY \\\n",
    "        -I $TUTORIAL_DATA_PATH/NA19017.chr20sub.bam \\\n",
    "        --format TSV \\\n",
    "        -O $WORKSPACE_LOCAL/sandbox/NA19017.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a TSV format table of read counts. Let's take a look. After the SAM format header section, denoted by lines starting with `@`, the body of the data has a column header line followed by read counts for every interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep -v '^@' $WORKSPACE_LOCAL/sandbox/NA19017.tsv | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on select parameters of _CollectReadCounts_\n",
    "\n",
    "- The tutorial generates text-based TSV (tab-separated-value) format data instead of the default HDF5 format by adding `--format TSV` to the command. Omit this option to generate the default HDF5 format. Downstream tools process HDF5 format more efficiently.\n",
    "\n",
    "- Here and elsewhere in the workflow, set `--interval-merging-rule` (`-imr`) to `OVERLAPPING_ONLY`, to prevent the tool from merging abutting intervals.\n",
    "\n",
    "- The tool employs a number of engine-level read filters. Of note are _NotDuplicateReadFilter_ and _MappingQualityReadFilter_. This means the tool excludes reads marked as duplicate and excludes reads with mapping quality less than `10`. Change the mapping quality threshold with the `--minimum-mapping-quality` option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate intervals with features using _AnnotateIntervals_ and subset regions of interest using _FilterIntervals_\n",
    "\n",
    "The steps in this section pertain to the **cohort mode**.\n",
    "\n",
    "Researchers may desire to subset the intervals that _GermlineCNVCaller_ will analyze, either to exclude potentially problematic regions or to retain only regions of interest. For example one may wish to exclude regions where all samples in a large cohort have copy number zero, or regions around segmental duplications (low-copy repeats) that often harbor common CNVs (e.g. in case the researcher is primarily interested in _de novo_ CNVs). Filtering intervals can be especially impactful for analyses that utilize references other than GRCh38 or that are based on sequencing technologies affected by sequence context, e.g. targeted exomes. The tutorial data is WGS data aligned to GRCh38, and the gCNV workflow can process the entirety of the data, without the need for any interval filtering.\n",
    "\n",
    "Towards deciding which regions to exclude, _AnnotateIntervals_ labels the given intervals with GC content and additionally with mappability and segmental duplication content if given the respective optional resource files. _FilterIntervals_ then subsets the intervals list based on the annotations and other tunable thresholds. Later, _GermlineCNVCaller_ also takes in the annotated intervals to use as covariates towards analysis.\n",
    "\n",
    "Explicit GC-correction, although optional, is recommended. The default v4.1.0.0 `cnv_germline_cohort_workflow.wdl` pipeline workflow omits explicit gc-correction and we activate it in the pipeline by setting `do_explicit_gc_correction\":\"True\"`. The tutorial illustrates the optional _AnnotateIntervals_ step by performing the recommended explicit GC-content-based filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _AnnotateIntervals_ with GC content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk AnnotateIntervals \\\n",
    "        -L $WORKSPACE_LOCAL/sandbox/chr20Y.interval_list \\\n",
    "        -R $REFERENCE_BUCKET/Homo_sapiens_assembly38.fasta \\\n",
    "        -imr OVERLAPPING_ONLY \\\n",
    "        -O $WORKSPACE_LOCAL/sandbox/chr20Y.annotated.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a four-column table where the fourth column gives the fraction of GC content. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep -v '^@' $WORKSPACE_LOCAL/sandbox/chr20Y.annotated.tsv | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on select parameters of _AnnotateIntervals_\n",
    "\n",
    "- The tool requires the `-R` reference and the `-L` intervals. The tool calculates GC-content for the intervals using the reference. Although optional for the tool, we recommend annotating mappability by providing a `--mappability-track` regions file in either .bed or .bed.gz format. Be sure to merge any overlapping intervals beforehand. The tutorial omits use of this resource.\n",
    "\n",
    "- GATK recommends use of the the single-read mappability track, as the multi-read track requires much longer times to process. For example, the Hoffman lab at the University of Toronto provides human and mouse mappability BED files for various kmer lengths at https://bismap.hoffmanlab.org. The accompanying publication is titled [Umap and Bismap: quantifying genome and methylome mappability](https://doi.org/10.1093/nar/gky677).\n",
    "\n",
    "- Optionally and additionally, annotate segmental duplication content by providing a `--segmental-duplication-track` regions file in either .bed or .bed.gz format. \n",
    "\n",
    "- Exclude undesirable intervals with the `-XL` parameter, e.g. intervals corresponding to centromeric regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _FilterIntervals_ based on GC-content and cohort extreme counts\n",
    "\n",
    "_FilterIntervals_ takes preprocessed intervals and either annotated intervals or read counts or both. It can also exclude intervals given with `-XL`. When given both types of data, the tool retains the intervals that intersect from filtering on each data type. The v4.1.0.0 `cnv_germline_cohort_workflow.wdl` pipeline script requires read counts files, and so by default the pipeline script always performs the _FilterIntervals_ step on read counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk FilterIntervals \\\n",
    "        -L $WORKSPACE_LOCAL/sandbox/chr20Y.interval_list \\\n",
    "        --annotated-intervals $WORKSPACE_LOCAL/sandbox/chr20Y.annotated.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG00096.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG00268.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG00419.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG00759.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG01051.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG01112.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG01500.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG01565.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG01583.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG01595.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG01879.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG02568.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG02922.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG03006.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG03052.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG03642.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/HG03742.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18525.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18939.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19017.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19625.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19648.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20502.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20845.tsv \\\n",
    "        -imr OVERLAPPING_ONLY \\\n",
    "        -O $WORKSPACE_LOCAL/sandbox/chr20Y.cohort.gc.filtered.interval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep -v '^@' $WORKSPACE_LOCAL/sandbox/chr20Y.interval_list | wc -l\n",
    "! grep -v '^@' $WORKSPACE_LOCAL/sandbox/chr20Y.cohort.gc.filtered.interval_list | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a Picard-style intervals list containing a subset of the starting intervals (80,534 of 87,641)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on select parameters of _FilterIntervals_\n",
    "\n",
    "- The tool requires the preprocessed intervals, provided with `-L`, from the previous section. Given annotated intervals with `--annotated-intervals`, the tool filters intervals on the given annotation(s).\n",
    "\n",
    "- GC-content thresholds are set by `--minimum-gc-content` and `--maximum-gc-content`, where defaults are `0.1` and `0.9`, respectively.\n",
    "\n",
    "- Mappability thresholds are set by `--minimum-mappability` and `--maximum-mappability`. Defaults are `0.9` and `1.0`, respectively.\n",
    "\n",
    "- Segmental duplication content thresholds are set by `--minimum-segmental-duplication-content` and `--maximum-segmental-duplication-content`. Defaults are 0.0 and 0.5, respectively.\n",
    "\n",
    "- Given read counts files, each with `-I` and in either HDF5 or TSV format, the tool filters intervals on low and extreme read counts with the following tunable thresholds.\n",
    "    - `--low-count-filter-count-threshold` default is `5`\n",
    "    - `--low-count-filter-percentage-of-samples` default is `90.0`\n",
    "    - `--extreme-count-filter-minimum-percentile` default is `1.0`\n",
    "    - `--extreme-count-filter-maximum-percentile` default is `99.0`\n",
    "    - `--extreme-count-filter-percentage-of-samples` default is `90.0`    \n",
    "The read counts data must match each other in intervals. For the default parameters, the tool first filters intervals with a count less than `5` in greater than `90%` of the samples. The tool then filters the remaining intervals with a count percentile less than `1` or greater than `99` in a percentage of samples greater than `90%`. These parameters effectively exclude intervals where all samples have extreme outlier counts, e.g. are deleted.\n",
    "\n",
    "- To disable counts based filtering, omit the read counts or, e.g. when using the v4.1.0.0 _cnv_germline_cohort_workflow.wdl_ pipeline script, set the two percentage-of-samples parameters as follows.\n",
    "    - `--low-count-filter-percentage-of-samples 100`\n",
    "    - `--extreme-count-filter-percentage-of-samples 100`    \n",
    "\n",
    "- Provide intervals to exclude from analysis with `--exclude-intervals` or `-XL`, e.g. centromeric regions. A frugal strategy is to collect read counts using the entirety of intervals and then to exclude undesirable regions later at the _FilterIntervals_ step (section 3), the _DetermineGermlineContigPloidy_ step (section 4), at the _GermlineCNVCaller_ step (section 6) and/or post-calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call autosomal and allosomal contig ploidy with _DetermineGermlineContigPloidy_\n",
    "\n",
    "_DetermineGermlineContigPloidy_ calls contig level ploidies (i.e. copy number) for both autosomal, e.g. human chr20, and allosomal contigs, e.g. human chrX. The tool determines baseline contig ploidies using sample coverages and contig ploidy priors that give the prior probabilities for each ploidy state for each contig. In this process, the tool generates global baseline coverage and noise data _GermlineCNVCaller_ will use in section 6.\n",
    "\n",
    "The tool determines baseline contig ploidies using the total read count per contig. Researchers should consider the impact of this for their data. For example, for the tutorial WGS data, the contribution of the PAR regions to total coverage counts on chrX is small and the tool correctly calls allosomal ploidies. However, consider blacklisting PAR regions for data where the contribution is disporportionate, e.g. targeted panels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _DetermineGermlineContigPloidy_ in cohort mode\n",
    "\n",
    "The cohort mode requires a `--contig-ploidy-priors` table and produces a ploidy model. The ploidy prior table specifies the prior probability of various ploidy states is formatted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat $TUTORIAL_DATA_PATH/chr20XY_contig_ploidy_priors.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this table, we put 98% prior probability to call chr20 as diploid (PLOIDY_PRIOR_2), 0% prior probability to call it homozygous deletion (would you be alive without chr20?), and a small probability 1% each for heterozygous deletion (PLOIDY_PRIOR_1) and trisomy (PLOIDY_PRIOR_3).\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --max-training-epochs 20 --num-thermal-advi-iters 1000 --max-advi-iter-subsequent-epochs 500 args only used \n",
    "# for tutorial to decreaase training time. Not recommeneded for use with full size data.\n",
    "\n",
    "! gatk DetermineGermlineContigPloidy \\\n",
    "        -L $WORKSPACE_LOCAL/sandbox/chr20Y.cohort.gc.filtered.interval_list \\\n",
    "        --interval-merging-rule OVERLAPPING_ONLY \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18525.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18939.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19017.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19625.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19648.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20502.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20845.tsv \\\n",
    "        --contig-ploidy-priors $TUTORIAL_DATA_PATH/chr20XY_contig_ploidy_priors.tsv \\\n",
    "        --max-training-epochs 20 --num-thermal-advi-iters 1000 --max-advi-iter-subsequent-epochs 500 \\\n",
    "        --output $WORKSPACE_LOCAL/sandbox \\\n",
    "        --output-prefix ploidy-cohort7 \\\n",
    "        --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a `<output-prefix>-calls` directory and a `<output-prefix>-model` directory.\n",
    "\n",
    "The ploidy-calls directory contains a folder of data for each sample in the cohort including the contig ploidy calls. Each sample directory, e.g. `ploidy-calls/SAMPLE_0`, contains five files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/ploidy-cohort7-calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/ploidy-cohort7-calls/SAMPLE_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the ploidy calls for NA19017\n",
    "! cat $WORKSPACE_LOCAL/sandbox/ploidy-cohort7-calls/SAMPLE_2/sample_name.txt\n",
    "! cat $WORKSPACE_LOCAL/sandbox/ploidy-cohort7-calls/SAMPLE_2/contig_ploidy.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of output files:\n",
    "- `contig_ploidy.tsv` notes the ploidy and genotype quality (GQ) of the ploidy call for each contig.\n",
    "- `global_read_depth.tsv` notes an average depth value and an average ploidy across all the intervals of the sample.\n",
    "- `mu_psi_s_log__.tsv` captures the posterior mean for all of the modeled parameters.\n",
    "- `sample_name.txt` contains the readgroup sample (RG SM) name.\n",
    "- `std_psi_s_log__.tsv` captures the standard deviation for all of the modeled paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/ploidy-cohort7-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ploidy-model directory contains aggregated model data for the cohort. This is the model to provide to a case-mode _DetermineGermlineContigPloidy_ analysis and to _GermlineCNVCaller_. The tutorial ploidy-model directory contains the eight files as follows.\n",
    "\n",
    "- `contig_ploidy_prior.tsv` is a copy of the ploidy priors given to the tool.\n",
    "- `gcnvkernel_version.json` notes the version of the kernel.\n",
    "- `interval_list.tsv` recapitulates the intervals used, e.g. the filtered intervals.\n",
    "- `mu_mean_bias_j_lowerbound__.tsv` is the estimated contig-level capture bias (mean).\n",
    "- `mu_psi_j_log__.tsv` is the estimated contig-level capture noise (mean).\n",
    "- `ploidy_config.json` configuration of the ploidy model\n",
    "- `std_mean_bias_j_lowerbound__.tsv` is the estimated contg-level capture bias (std).\n",
    "- `std_psi_j_log__.tsv` is the estimated contig-level capture noise (std).   \n",
    "\n",
    "Note: The PyMC3/Theano model automatically generates mu_ and std_ files and may append transformations it performs to the file name, e.g. log or lowerbound as we see above. These are likely of interest only to advanced users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for the next section (i.e. running the tool in case mode), let us run _DetermineGermlineContigPloidy_ one more time, but assuming that we didn't have `NA19017` sample in the original cohort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --max-training-epochs 20 --num-thermal-advi-iters 1000 --max-advi-iter-subsequent-epochs 500 args only used \n",
    "# for tutorial to decrease training time. Not recommeneded for use with full size data.\n",
    "\n",
    "! gatk DetermineGermlineContigPloidy \\\n",
    "        -L $WORKSPACE_LOCAL/sandbox/chr20Y.cohort.gc.filtered.interval_list \\\n",
    "        --interval-merging-rule OVERLAPPING_ONLY \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18525.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18939.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19625.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19648.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20502.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20845.tsv \\\n",
    "        --contig-ploidy-priors $TUTORIAL_DATA_PATH/chr20XY_contig_ploidy_priors.tsv \\\n",
    "        --max-training-epochs 20 --num-thermal-advi-iters 1000 --max-advi-iter-subsequent-epochs 500 \\\n",
    "        --output $WORKSPACE_LOCAL/sandbox \\\n",
    "        --output-prefix ploidy-cohort6 \\\n",
    "        --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _DetermineGermlineContigPloidy_ in case mode\n",
    "\n",
    "The case mode calls contig ploidies for each sample against the ploidy model given by `--model`. The following command runs sample NA19017 against the 6-sample cohort model we built above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --max-training-epochs 20 --num-thermal-advi-iters 1000 --max-advi-iter-subsequent-epochs 500 args only used \n",
    "# for tutorial to decrease training time. Not recommeneded for use with full size data.\n",
    "\n",
    "! gatk DetermineGermlineContigPloidy \\\n",
    "        --model $WORKSPACE_LOCAL/sandbox/ploidy-cohort6-model \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19017.tsv \\\n",
    "        -O $WORKSPACE_LOCAL/sandbox \\\n",
    "        --output-prefix ploidy-cohort6-case \\\n",
    "        --max-training-epochs 20 --num-thermal-advi-iters 1000 --max-advi-iter-subsequent-epochs 500 \\\n",
    "        --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a ploidy-case-calls directory, which in turn contains a directory of sample data, SAMPLE_0. A list of the five resulting files is some paragraphs above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/ploidy-cohort6-case-calls/SAMPLE_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the ploidy calls for the case sample (NA19017)\n",
    "! cat $WORKSPACE_LOCAL/sandbox/ploidy-cohort6-case-calls/SAMPLE_0/sample_name.txt\n",
    "! cat $WORKSPACE_LOCAL/sandbox/ploidy-cohort6-case-calls/SAMPLE_0/contig_ploidy.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on select parameters of _DetermineGermlineContigPloidy_\n",
    "\n",
    "- It is possible to analyze multiple samples simultaneously in a case mode command. Provide each sample with `-I`.\n",
    "- For the `-L` intervals, supply the most processed intervals list. For the tutorial, this is the filtered intervals. Note the case mode does not require explicit intervals because the ploidy model provides them.\n",
    "- Provide a `--contig-ploidy-priors` table containing the per-contig prior probabilities for integer ploidy state. Again, the case mode does not require an explicit priors file as the ploidy model provides them. Tool index describes this resource in detail.\n",
    "- Optionally provide intervals to exclude from analysis with `--exclude-intervals` or `-XL`, e.g. [pseudoautosomal (PAR) regions](https://gatk.zendesk.com/hc/en-us/articles/360035891071), which can skew the results on sex chromosomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call copy-number variants with _GermlineCNVCaller_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_GermlineCNVCaller_ learns a denoising model per scattered shard while consistently calling CNVs across the shards. The tool models systematic biases and CNVs simultaneously, which allows for sensitive detection of both rare and common CNVs. As the tool index states under Important Remarks (v4.1.0.0), the tool should see data from a large enough genomic region so as to be exposed to diverse genomic features. The current recommendation is to provide at least ~10–50Mbp genomic coverage per scatter. This applies to exomes or WGS. This allows reliable inference of bias factors including GC bias. The limitation of analyzing larger regions is available memory. As an analysis covers more data, memory requirements increase.\n",
    "\n",
    "For expediency, the tutorial commands below analyze small data, specifically the 1400 bins in `twelveregions.cohort.gc.filtered.interval_list` and use default parameters. The tutorial splits the 1400 bins into two shards with 700 bins each to illustrate scattering. This results in ~0.7Mbp genomic coverage per shard. See section 5.2.3 for how to split interval lists by a given number of intervals. Default inference parameters are conservatively set for efficient run times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _GermlineCNVCaller_ in cohort mode\n",
    "\n",
    "This cell produces per-interval gCNV calls for each of the cohort samples and a gCNV model for the cohort. Each command produces three directories within `gcnv-cohort7-twelve`:\n",
    "- a `gcnv-cohort7-twelve-1of2-calls` folder of per sample gCNV call results,\n",
    "- a `gcnv-cohort7-twelve-1of2-model` folder of cohort model data,\n",
    "- a `gcnv-cohort7-twelve-1of2-tracking` folder of data that tracks model fitting.\n",
    "\n",
    "Note: Each shard should finish under 10 minutes in this Terra VM (4 vCPUs).  When running the default parameters of the v4.1.0.0 WDL cohort-mode workflow on the cloud, the majority of the shard analyses complete in half an hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First shard (7 samples)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk GermlineCNVCaller \\\n",
    "        --run-mode COHORT \\\n",
    "        -L $TUTORIAL_DATA_PATH/scatter-sm/twelve_1of2.interval_list \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18525.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18939.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19017.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19625.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19648.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20502.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20845.tsv \\\n",
    "        --contig-ploidy-calls $WORKSPACE_LOCAL/sandbox/ploidy-cohort7-calls \\\n",
    "        --annotated-intervals $TUTORIAL_DATA_PATH/twelveregions.annotated.tsv \\\n",
    "        --interval-merging-rule OVERLAPPING_ONLY \\\n",
    "        --output $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve \\\n",
    "        --output-prefix gcnv-cohort7-twelve-1of2 \\\n",
    "        --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second shard (7 samples)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk GermlineCNVCaller \\\n",
    "        --run-mode COHORT \\\n",
    "        -L $TUTORIAL_DATA_PATH/scatter-sm/twelve_2of2.interval_list \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18525.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18939.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19017.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19625.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19648.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20502.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20845.tsv \\\n",
    "        --contig-ploidy-calls $WORKSPACE_LOCAL/sandbox/ploidy-cohort7-calls \\\n",
    "        --annotated-intervals $TUTORIAL_DATA_PATH/twelveregions.annotated.tsv \\\n",
    "        --interval-merging-rule OVERLAPPING_ONLY \\\n",
    "        --output $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve \\\n",
    "        --output-prefix gcnv-cohort7-twelve-2of2 \\\n",
    "        --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get acquainted with the output\n",
    "! ls $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve/gcnv-cohort7-twelve-1of2-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(advanced)** The model directory contains aggregated model data for the cohort and the analyzed interval shard. This is the model to provide to a case-mode _GermlineCNVCaller_. The model files are as follows:\n",
    "\n",
    "- `calling_config.json` is the configuration of the copy-number caller sub-model (the two-level HMM)\n",
    "- `denoising_config.json` is the configuration of the read-count noise model\n",
    "- `gcnvkernel_version.json` notes the version of the gcnvkernel\n",
    "- `interval_list.tsv` recapitulates the intervals used in this shard\n",
    "- `log_q_tau_tk.tsv` is the posterior probability of region class (rare vs. common) in log-scale\n",
    "- `mu_ard_u_log__.tsv` is ARD coefficient of the bias factors in log-scale (mean)\n",
    "- `mu_log_mean_bias_t.tsv` is the mean read-count capture bias of each of interval (mean) \n",
    "- `mu_psi_t_log__.tsv` is the read-count unexplained variance (overdispersion) of each of interval in log-scale (mean)\n",
    "- `mu_W_tu.tsv` is the interval x number of bias factors matrix of learned bias factors (mean)\n",
    "- `std_ard_u_log__.tsv` is ARD coefficient of the bias factors in log-scale (std)\n",
    "- `std_log_mean_bias_t.tsv` is the mean read-count capture bias of each of interval (std) \n",
    "- `std_psi_t_log__.tsv` is the read-count unexplained variance (overdispersion) of each of interval in log-scale (std)\n",
    "- `std_W_tu.tsv` is the interval x number of bias factors matrix of learned bias factors (std)   \n",
    "Note: The PyMC3/Theano model automatically generates mu_ and std_ prefixed files and may append transformations it performs to the file name (e.g. log) as we see above. These are likely of interest only to advanced users who may want to inspect the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve/gcnv-cohort7-twelve-1of2-tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(advanced)** Loss function trajectory during model training:\n",
    "\n",
    "- `warm_up_elbo_history.tsv` is evidence lower bound (ELBO) history during the initial model warm-up period\n",
    "- `main_elbo_history.tsv` is evidence lower bound (ELBO) history during the main model training period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve/gcnv-cohort7-twelve-1of2-calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve/gcnv-cohort7-twelve-1of2-calls/SAMPLE_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(advanced)** The calls directory contains various sample-specific estimated quantities, including per-interval estimated copy-number states. These intermediate results are later post-processed by `PostprocessGermlineCNVCalls` to produce VCF files (see the next section). The per-sample call files are as follows:\n",
    "\n",
    "- `baseline_copy_number_t.tsv` is the per-interval baseline copy-number of the sample (recapitulated from DetermineGermlineContigPloidy)\n",
    "- `log_c_emission_tc.tsv` is the emission probability of each interval to different copy-number states in log-scale\n",
    "- `log_q_c_tc.tsv` is the per-interval posterior probability of each copy-number state in log-scale\n",
    "- `mu_denoised_copy_ratio_t.tsv` is the per-interval denoised copy-ratio\n",
    "- `mu_psi_s_log__.tsv` is the global unexplained variance (overdispersion) of this sample in log-scale (mean)\n",
    "- `mu_read_depth_s_log__.tsv` is the global read-depth of this sample in log-scale (mean)\n",
    "- `mu_z_sg.tsv` is loading of each GC bin (related to GC bias correction) (mean) \n",
    "- `mu_z_su.tsv` is the loading of each bias factor (mean)\n",
    "- `sample_name.txt` is the name of the sample\n",
    "- `std_denoised_copy_ratio_t.tsv` is the per-interval denoised copy-ratio\n",
    "- `std_psi_s_log__.tsv` is the global unexplained variance (overdispersion) of this sample in log-scale (std)\n",
    "- `std_read_depth_s_log__.tsv` is the global read-depth of this sample in log-scale (std)\n",
    "- `std_z_sg.tsv` is loading of each GC bin (related to GC bias correction) (std) \n",
    "- `std_z_su.tsv` is the loading of each bias factor (std)  \n",
    "Note: The PyMC3/Theano model automatically generates mu_ and std_ prefixed files and may append transformations it performs to the file name (e.g. log__) as we see above. These are likely of interest only to advanced users who may want to inspect the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test _GermlineCNVCaller_ in case mode, let us assume we didn't have sample `NA19017` in the initial cohort and let us build a cohort of 6 samples. We will then run _GermlineCNVCaller_ in case mode using the model learned from the 6-sample cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First shard (6 samples)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk GermlineCNVCaller \\\n",
    "        --run-mode COHORT \\\n",
    "        -L $TUTORIAL_DATA_PATH/scatter-sm/twelve_1of2.interval_list \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18525.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18939.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19625.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19648.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20502.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20845.tsv \\\n",
    "        --contig-ploidy-calls $WORKSPACE_LOCAL/sandbox/ploidy-cohort6-calls \\\n",
    "        --annotated-intervals $TUTORIAL_DATA_PATH/twelveregions.annotated.tsv \\\n",
    "        --interval-merging-rule OVERLAPPING_ONLY \\\n",
    "        --output $WORKSPACE_LOCAL/sandbox/gcnv-cohort6-twelve \\\n",
    "        --output-prefix gcnv-cohort6-twelve-1of2 \\\n",
    "        --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second shard (6 samples)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk GermlineCNVCaller \\\n",
    "        --run-mode COHORT \\\n",
    "        -L $TUTORIAL_DATA_PATH/scatter-sm/twelve_2of2.interval_list \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18525.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA18939.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19625.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19648.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20502.tsv \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA20845.tsv \\\n",
    "        --contig-ploidy-calls $WORKSPACE_LOCAL/sandbox/ploidy-cohort6-calls \\\n",
    "        --annotated-intervals $TUTORIAL_DATA_PATH/twelveregions.annotated.tsv \\\n",
    "        --interval-merging-rule OVERLAPPING_ONLY \\\n",
    "        --output $WORKSPACE_LOCAL/sandbox/gcnv-cohort6-twelve \\\n",
    "        --output-prefix gcnv-cohort6-twelve-2of2 \\\n",
    "        --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _GermlineCNVCaller_ in case mode\n",
    "\n",
    "Call gCNVs on a sample against a cohort model. The case analysis must use the same scatter approach as the model generation. So, as above, we run two shard analyses. Here, `--model` and `--output-prefix` differ between the scatter the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First shard (case-calling sample `NA19017` against the model built from the 6-sample cohort)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk GermlineCNVCaller \\\n",
    "        --run-mode CASE \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19017.tsv \\\n",
    "        --contig-ploidy-calls $WORKSPACE_LOCAL/sandbox/ploidy-cohort6-case-calls \\\n",
    "        --model $WORKSPACE_LOCAL/sandbox/gcnv-cohort6-twelve/gcnv-cohort6-twelve-1of2-model \\\n",
    "        --output $WORKSPACE_LOCAL/sandbox/gcnv-case-twelve-vs-cohort6 \\\n",
    "        --output-prefix gcnv-case-twelve-vs-cohort6-1of2 \\\n",
    "        --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second shard (case-calling sample `NA19017` against the model built from the 6-sample cohort)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk GermlineCNVCaller \\\n",
    "        --run-mode CASE \\\n",
    "        -I $TUTORIAL_DATA_PATH/cvg/NA19017.tsv \\\n",
    "        --contig-ploidy-calls $WORKSPACE_LOCAL/sandbox/ploidy-cohort6-case-calls \\\n",
    "        --model $WORKSPACE_LOCAL/sandbox/gcnv-cohort6-twelve/gcnv-cohort6-twelve-2of2-model \\\n",
    "        --output $WORKSPACE_LOCAL/sandbox/gcnv-case-twelve-vs-cohort6 \\\n",
    "        --output-prefix gcnv-case-twelve-vs-cohort6-2of2 \\\n",
    "        --verbosity DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get acquainted with the output\n",
    "! ls $WORKSPACE_LOCAL/sandbox/gcnv-case-twelve-vs-cohort6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/gcnv-case-twelve-vs-cohort6/gcnv-case-twelve-vs-cohort6-1of2-calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $WORKSPACE_LOCAL/sandbox/gcnv-case-twelve-vs-cohort6/gcnv-case-twelve-vs-cohort6-1of2-calls/SAMPLE_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call output in the case mode is similar to the cohort mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the workflow has done its most heavy lifting to produce data towards copy number calling. In Section 5, we consolidate the data from the scattered GermlineCNVCaller runs, perform segmentation and call copy number states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on select parameters of _GermlineCNVCaller_\n",
    "\n",
    "- The `-O` output directory must be extant before running the command. Future releases (v4.1.1.0) will create the directory.\n",
    "- The default `--max-copy-number` is capped at 5. This means the tool reports any events with more copies as CN5.\n",
    "- For the cohort mode, optionally provide `--annotated-intervals` to include the annotations as covariates. These must contain all of the `-L` intervals. The `-L` intervals is an exact match or a subset of the annotated intervals.\n",
    "- For the case mode, the tool accepts only a single `--model` directory at a time. So the case must be analyzed with the same number of scatters as the cohort model run. The case mode parameters appear fewer than the cohort mode because the `--model` directory provides the seemingly missing requirements, i.e. the scatter intervals and the annotated intervals.\n",
    "- For both modes, provide the `--contig-ploidy-calls` results from _DetermineGermlineContigPloidy_ (Section 4).\n",
    "- `--verbosity DEBUG` allows tracking the Python `gcnvkernel` model fitting in the stdout, e.g. with information on denoising epochs and whether the model converged. The default INFO level verbosity is the next most verbose and emits only GATK Engine level messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I increase the sensitivity of CNV detection?\n",
    "\n",
    "The tutorial uses default _GermlineCNVCaller_ modeling parameters. However, researchers should expect to tune parameters for data, e.g. from different sequencing technologies. For tuning, first consider the coherence length parameters, `p-alt`, `p-active` and the `psi-scale` parameters. These hyperparameters are just a few of the plethora of adjustable parameters _GermlineCNVCaller_ offers. Refer to the GermlineCNVCaller tool documentation for detailed explanations, and ask on the GATK Forum for further guidance.\n",
    "\n",
    "One set of parameter changes for WGS data that dramatically increase the sensitivity of calling on the tutorial data:\n",
    "```\n",
    "    --class-coherence-length 1000.0 \\\n",
    "    --cnv-coherence-length 1000.0 \\\n",
    "    --enable-bias-factors false \\\n",
    "    --interval-psi-scale 1.0E-6 \\\n",
    "    --log-mean-bias-standard-deviation 0.01 \\\n",
    "    --sample-psi-scale 1.0E-6 \\\n",
    "```\n",
    "\n",
    "Article #11687 and Notebook #11686 compare the results of using default vs. the increased-sensitivity parameters. Given the absence of off-the-shelf filtering solutions for CNV calls, when tuning parameters to increase sensitivity, researchers should expect to perform additional due diligence, especially for analyses requiring high precision calls.\n",
    "\n",
    "**Comments on select sensitivity parameters**\n",
    "\n",
    "- Decreasing `--class-coherence-length` from its default of `10,000bp` to `1000bp` decreases the expected length of contiguous segments of genomic region classes (silent vs. active). Factor for bin size when tuning.\n",
    "- Decreasing `--cnv-coherence-length` from its default `10,000bp` to `1000bp` decreases the expected length of per-sample CNV events. Factor for bin size when tuning.\n",
    "- Turning off `--enable-bias-factors` from the default true state to false turns off active discovery of learnable bias factors. This should always be on for targeted exome data and in general can be turned off for WGS data.\n",
    "- Decreasing `--interval-psi-scale` from its default of `0.001` to `1.0E-6` reduces the scale the tool considers normal in per-interval noise.\n",
    "- Decreasing `--log-mean-bias-standard-deviation` from its default of `0.1` to `0.01` reduces what is considered normal noise in capture bias.\n",
    "- Decreasing `--sample-psi-scale` from its default of `0.0001` to `1.0E-6` reduces the scale that is considered normal in sample-specific global read-count overdispersion.\n",
    "\n",
    "\n",
    "Additional parameters to consider include `--depth-correction-tau`, `--p-active` and `--p-alt`:\n",
    "\n",
    "- `--depth-correction-tau` has a default of `10000.0` (10K) and defines the precision of read-depth concordance with the prior estimate.\n",
    "- `--p-active` has a default of `1e-2` (0.01) and defines the prior probability of designating common CNV genomic regions.\n",
    "- `--p-alt` has a default of `1e-6` (0.000001) and defines the expected probability of CNV events (in silent genomic regions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I make interval lists for scattering (genomic sharding)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step applies to the cohort mode. It is unnecessary for case mode analyses as the model implies the scatter intervals.\n",
    "\n",
    "The v4.1.0.0 `cnv_germline_cohort_workflow.wdl` pipeline workflow scatters the _GermlineCNVCaller_ step. Each scattered analysis is on genomic intervals subset from intervals produced either from _PreprocessIntervals_ (section 2) or from _FilterIntervals_ (section 3). The workflow uses Picard _IntervalListTools_ to break up the intervals list into roughly balanced lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $WORKSPACE_LOCAL/sandbox/scatter\n",
    "! gatk IntervalListTools \\\n",
    "        --INPUT $TUTORIAL_DATA_PATH/chr20sub.cohort.gc.filtered.interval_list \\\n",
    "        --SUBDIVISION_MODE INTERVAL_COUNT \\\n",
    "        --SCATTER_CONTENT 5000 \\\n",
    "        --OUTPUT $WORKSPACE_LOCAL/sandbox/scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces three intervals lists with ~5K intervals each. For the tutorial's 1Kbp bins, this gives ~5Mbp genomic coverage per scatter. Each list is identically named scattered.interval_list within its own folder within the scatter directory. _IntervalListTools_ systematically names the intermediate folders, e.g. temp_0001_of_3, temp_0002_of_3 and temp_0002_of_3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on select parameters of _IntervalListTools_\n",
    "\n",
    "- The `--SUBDIVISION_MODE INTERVAL_COUNT` mode scatters intervals into similarly sized lists according to the count of intervals regardless of the base count. The tool intelligently breaks up the chr20sub.cohort.gc.filtered.interval_list's ~15K intervals into lists of 5031, 5031 and 5033 intervals. This is preferable to having a fourth interval list with just 95 intervals.\n",
    "- The tool has another useful feature in the context of the gCNV workflow. To subset `-I` binned intervals, provide the regions of interest with `-SI` (`--SECOND_INPUT`) and use the `--ACTION OVERLAPS` mode to create a new intervals list of the overlapping bins. Adding `--SUBDIVISION_MODE INTERVAL_COUNT --SCATTER_CONTENT 5000` will produce scatter intervals concurrently with the subsetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call copy number segments and consolidate sample results with _PostprocessGermlineCNVCalls_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_PostprocessGermlineCNVCalls_ consolidates the scattered _GermlineCNVCaller_ results, performs segmentation and calls copy number states. The tool generates per-interval and per-segment CNV calls in VCF format and runs on a single sample at a time. It also produces consolidated denoised copy-ratio estimates from all interval shards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _PostprocessGermlineCNVCalls_ in cohort mode\n",
    "\n",
    "Process a single sample from the 7-sample cohort using the sample index. For `NA19017`, the sample index is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk PostprocessGermlineCNVCalls \\\n",
    "        --model-shard-path $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve/gcnv-cohort7-twelve-1of2-model \\\n",
    "        --model-shard-path $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve/gcnv-cohort7-twelve-2of2-model \\\n",
    "        --calls-shard-path $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve/gcnv-cohort7-twelve-1of2-calls \\\n",
    "        --calls-shard-path $WORKSPACE_LOCAL/sandbox/gcnv-cohort7-twelve/gcnv-cohort7-twelve-2of2-calls \\\n",
    "        --allosomal-contig chrX --allosomal-contig chrY \\\n",
    "        --contig-ploidy-calls $WORKSPACE_LOCAL/sandbox/ploidy-cohort7-calls \\\n",
    "        --sample-index 2 \\\n",
    "        --output-genotyped-intervals $WORKSPACE_LOCAL/sandbox/genotyped-intervals-cohort7-twelve-NA19017.vcf.gz \\\n",
    "        --output-genotyped-segments $WORKSPACE_LOCAL/sandbox/genotyped-segments-cohort7-twelve-NA19017.vcf.gz \\\n",
    "        --output-denoised-copy-ratios $WORKSPACE_LOCAL/sandbox/denoised-copy-ratios-cohort7-twelve-NA19017.tsv \\\n",
    "        --sequence-dictionary $REFERENCE_BUCKET/Homo_sapiens_assembly38.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _PostprocessGermlineCNVCalls_ in case mode\n",
    "\n",
    "`NA19017` is the singular sample with index 0 in the case mode run from section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk PostprocessGermlineCNVCalls \\\n",
    "        --model-shard-path $WORKSPACE_LOCAL/sandbox/gcnv-cohort6-twelve/gcnv-cohort6-twelve-1of2-model \\\n",
    "        --model-shard-path $WORKSPACE_LOCAL/sandbox/gcnv-cohort6-twelve/gcnv-cohort6-twelve-2of2-model \\\n",
    "        --calls-shard-path $WORKSPACE_LOCAL/sandbox/gcnv-case-twelve-vs-cohort6/gcnv-case-twelve-vs-cohort6-1of2-calls \\\n",
    "        --calls-shard-path $WORKSPACE_LOCAL/sandbox/gcnv-case-twelve-vs-cohort6/gcnv-case-twelve-vs-cohort6-2of2-calls \\\n",
    "        --allosomal-contig chrX --allosomal-contig chrY \\\n",
    "        --contig-ploidy-calls $WORKSPACE_LOCAL/sandbox/ploidy-cohort6-case-calls \\\n",
    "        --sample-index 0 \\\n",
    "        --output-genotyped-intervals $WORKSPACE_LOCAL/sandbox/genotyped-intervals-case-twelve-vs-cohort6.vcf.gz \\\n",
    "        --output-genotyped-segments $WORKSPACE_LOCAL/sandbox/genotyped-segments-case-twelve-vs-cohort6.vcf.gz \\\n",
    "        --output-denoised-copy-ratios $WORKSPACE_LOCAL/sandbox/denoised-copy-ratios-cohort6-twelve-NA19017.tsv \\\n",
    "        --sequence-dictionary $REFERENCE_BUCKET/Homo_sapiens_assembly38.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each command generates two VCFs with indices, and a TSV file. The `genotyped-intervals` VCF contains variant records for each genomic interval and therefore data covers only the interval regions. For the tutorial's small data, this gives 1400 records. The `genotyped-segments` VCF contains records for each contiguous copy number state segment. For the tutorial's small data, this is 30 and 31 records for cohort and case mode analyses, respectively. Finally, the `denoised-copy-ratios` TSV contains the denoised copy-ratio estimates for each genomic interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNV calls for sample `NA19017` are expected to be highly concordant between cohort and case modes. The slight difference is due to the contribution of the sample itself to model training, which is absent in the 6-sample model used for case-calling the sample. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's exact a few things from the VCF file to a nice table\n",
    "! gatk VariantsToTable \\\n",
    "        -V $WORKSPACE_LOCAL/sandbox/genotyped-intervals-case-twelve-vs-cohort6.vcf.gz \\\n",
    "        -F CHROM -F POS -F END -GF CN \\\n",
    "        -O $WORKSPACE_LOCAL/sandbox/genotyped-intervals-case-twelve-vs-cohort6.table.txt\n",
    "! gatk VariantsToTable \\\n",
    "        -V $WORKSPACE_LOCAL/sandbox/genotyped-intervals-cohort7-twelve-NA19017.vcf.gz \\\n",
    "        -F CHROM -F POS -F END -GF CN \\\n",
    "        -O $WORKSPACE_LOCAL/sandbox/genotyped-intervals-cohort7-twelve-NA19017.table.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_copy_number_table(table_file_path):\n",
    "    table_data = []\n",
    "    with open(table_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.split()[0] == 'CHROM':\n",
    "                pass\n",
    "            else:\n",
    "                line_tokens = line.strip().split()\n",
    "                start_pos = int(line_tokens[1])\n",
    "                end_pos = int(line_tokens[2])\n",
    "                cn = int(line_tokens[3])\n",
    "                table_data.append((start_pos, end_pos, cn))\n",
    "    return table_data\n",
    "\n",
    "\n",
    "cohort_cn_calls_data = load_copy_number_table(\n",
    "    f'{WORKSPACE_LOCAL}/sandbox/genotyped-intervals-cohort7-twelve-NA19017.table.txt')\n",
    "\n",
    "case_cn_calls_data = load_copy_number_table(\n",
    "    f'{WORKSPACE_LOCAL}/sandbox/genotyped-intervals-case-twelve-vs-cohort6.table.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cohort_cn_calls_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(case_cn_calls_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agreeing_intervals = sum(\n",
    "    cohort_cn_call[2] == case_cn_call[2]\n",
    "    for cohort_cn_call, case_cn_call in zip(cohort_cn_calls_data, case_cn_calls_data))\n",
    "\n",
    "print(n_agreeing_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only copy-number calls over 13 intervals differ between cohort and case mode for sample `NA19017`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to go from here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Article #11687](https://gatk.zendesk.com/hc/en-us/articles/360035531452) visualizes the results in IGV and provides follow-up discussion.\n",
    "\n",
    "Towards data exploration, here are two illustrative Jupyter Notebook reports that dissect the results:\n",
    "- [Notebook #11685](https://gatk.zendesk.com/hc/en-us/articles/360035890031/) shows an approach to measuring concordance of sample NA19017 gCNV calls to 1000 Genomes Project truth set calls using tutorial chr20sub small data.\n",
    "- [Notebook #11686](https://gatk.zendesk.com/hc/en-us/articles/360035889891) examines gCNV callset annotations using larger data, namely chr20 gCNV results using a 24-sample cohort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of 2-somatic-cna-tutorial.ipynb",
   "provenance": [
    {
     "file_id": "1Mq1TdSLhOasgxxz-CIDgeZlA_sKdotTs",
     "timestamp": 1560256061207
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "150px",
    "width": "161px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.055px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
