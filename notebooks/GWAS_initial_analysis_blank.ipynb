{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "----\n",
    "\n",
    "This notebook demonstrates typical initial steps in a genetic association analysis: exploring phenotype distributions, filtering and LD-pruning, and Principal Component analysis. It has been written to be interactive, allowing you to make choices as you go.  \n",
    "\n",
    "You should treat this notebook as a real analysis, where you will need to carefully consider aspects of the dataset to accurately model genotype-phenotype relationships. In particular, you will want to identify covariation of traits and population structure. These two aspects are among the most common causes of confounding within a GWAS.\n",
    "\n",
    "By the end of this notebook, you should understand:  \n",
    "\n",
    "1. How to import phenotypic data from a Terra data table to a notebook compute environment  \n",
    "\n",
    "2. Exploring and processing these data to understand their underlying structure  \n",
    "3. Defining an outcome and a set of covariates to use when modeling genotype-phenotype associations  \n",
    "\n",
    "4. Importing, exploring, and performing quality control on genotype data \n",
    "\n",
    "5. Understanding population structure within the 1000 Genomes sample   \n",
    "\n",
    "6. How to prepare a full set of input parameters and data for a genomewide association analysis pipeline  \n",
    "\n",
    "7. How to configure the Terra data model to easily run the association pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data disclaimer <a class=\"tocSkip\">\n",
    "\n",
    "All data in this notebook (and this workspace) are publicly available thanks to the effort of many dedicated individuals: \n",
    "\n",
    "- Genotype and some phenotypic data were produced by the [1000 Genomes Project (phase 3)](https://www.internationalgenome.org/)\n",
    "\n",
    "- Individual phenotypes were modeling using the [GCTA software](cnsgenomics.com/software/gcta) and variant-level summary statistics from [MAGIC](https://www.magicinvestigators.org/), the [GIANT Consortium](https://portals.broadinstitute.org/collaboration/giant/index.php/Main_Page), the [UK Biobank](https://www.ukbiobank.ac.uk/), and the [MVP](https://www.research.va.gov/mvp/)  \n",
    "\n",
    "Phenotypes were modeled to reflect the actual genetic architecture of these complex traits as closely as possible. Most single variant association results should correspond well to published GWAS, but others may not. **Results produced from these data should not be taken as representing real, replicable genetic associations. These data are provided for demonstration and training purposes only.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up your notebook\n",
    "----\n",
    "\n",
    "## Set runtime values\n",
    "If you are opening this notebook for the first time, and you did not edit your runtime settings before starting it, you will now need to change your settings. Click on the gear icon in the upper right to edit your Notebook Runtime. Set the values as specified below: \n",
    "\n",
    "<table style=\"float:left\">\n",
    "    <thead>\n",
    "        <tr><th>Option</th><th>Value</th></tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "         <tr><td>ENVIRONMENT</td><td>Hail: (Python 3.7.6, hail 0.2.30)</td></tr>\n",
    "         <tr><td>COMPUTE Profile</td><td>Custom</td></tr>   \n",
    "         <tr><td>CPUs</td><td>8</td></tr>     \n",
    "         <tr><td>Memory</td><td>30 GB</td></tr>    \n",
    "         <tr><td>Disk size</td><td>100</td></tr>   \n",
    "         <tr><td>Startup Script</td><td>leave blank</td></tr>  \n",
    "         <tr><td>Runtime Type</td><td>Configure as Spark Cluster</td></tr>\n",
    "         <tr><td>Workers</td><td>4</td></tr> \n",
    "         <tr><td>Preemptible</td><td>0</td></tr>     \n",
    "         <tr><td>Workers CPUs</td><td>4</td></tr>   \n",
    "         <tr><td>Workers Memory</td><td>15 GB</td></tr>\n",
    "         <tr><td>Workers Disk size</td><td>50 GB</td></tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Click the \"Replace\" button when you are done, and Terra will begin to create a new runtime with your settings. When it is finished, it will pop up asking you to apply the new settings.  \n",
    "\n",
    "## Check kernel type  \n",
    "\n",
    "A kernel is a _computational engine_ that executes the code in the notebook. You can think of it as defining the programming language. For this notebook, we'll use a `Python 3` kernel. In the upper right corner of the notebook, just under the Notebook Runtime, it should say `Python 3`. If it doesn't, you can switch it by navigating to the Kernel menu and selecting `Change kernel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python packages\n",
    "----\n",
    "\n",
    "* **FISS** - a toolkit for using Terra APIs through Python  \n",
    "* **Pandas & Numpy** - packages for data analysis  \n",
    "* **Pprint** - for pretty printing  \n",
    "* **Matplotlib & Seaborn** - for plotting  \n",
    "\n",
    "To see the entire list of Python packages, click the purple arrow to the right below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "from firecloud import fiss\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load phenotypes \n",
    "----\n",
    "\n",
    "Phenotypic data for each individual in the study are stored in the workspace data table. To analyze inside this notebook, we have to explicitly load the data in our notebook environment. To do this, we'll need some information about the Terra Workspace. This can be access programmatically using some environmental variables.  \n",
    "\n",
    "\n",
    "## Goal of this section<a class=\"tocSkip\">\n",
    "\n",
    "1. Use the FISS package to import data referenced in the workspace data table into the notebook environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get the Google billing project name and workspace name\n",
    "billing_project = os.environ['WORKSPACE_NAMESPACE']\n",
    "#workspace = os.environ['WORKSPACE_NAME']\n",
    "workspace=os.path.basename(os.path.dirname(os.getcwd()))\n",
    "bucket = os.environ['WORKSPACE_BUCKET'] + \"/\"\n",
    "\n",
    "# Verify that we've captured the environment variables\n",
    "print(\"Billing project: \" + billing_project)\n",
    "print(\"Workspace: \" + workspace)\n",
    "print(\"Workspace storage bucket: \" + bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load phenotype data\n",
    "\n",
    "We'll use <font color='red'>read_csv</font> to load the phenotypes from the workspace data table. The columns correspond to:  \n",
    "\n",
    "* **sample:** a unique label for each individual sample in our dataset\n",
    "* **age:** numerical age of the individual at the time of each phenotype measure\n",
    "* **ancestry:** superpopulation group of each individual\n",
    "  * AFR: African\n",
    "  * AMR: Ad Mixed American\n",
    "  * EAS: East Asian\n",
    "  * EUR: European\n",
    "  * SAS: South Asian\n",
    "* **bmi:** body mass index\n",
    "* **fg:** fasting glucose\n",
    "* **fi:** fasting insulin\n",
    "* **hdl:** high density lipoprotein\n",
    "* **height:** standing height\n",
    "* **ldl:** low density lipoprotein\n",
    "* **population:** population of each sample, see [1000 Genomes description](https://www.internationalgenome.org/category/population/)\n",
    "* **sex:** biological sex\n",
    "* **tc:** total cholesteral\n",
    "* **tg:** total triglycerides\n",
    "* **whr:** waist-to-hip ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the phenotype data and load into a pandas dataframe\n",
    "samples = pd.read_csv(io.StringIO(fiss.fapi.get_entities_tsv(billing_project, workspace, \"sample\").text), sep='\\t')\n",
    "samples.rename(columns = {'entity:sample_id':'sample'}, inplace = True)\n",
    "\n",
    "# Take a look at the top of our table\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine phenotype data\n",
    "----\n",
    "\n",
    "Now let's take a look at the phenotype distributions. In a GWAS - and statistical genetics more generally - we should always be on the lookout for correlations within our dataset. Correlations between phenotypic values can confound our analysis, leading to results that may not represent true genetic associations with our traits. Exploring these relationships may help in choosing a reasonable set of covariates to model.    \n",
    "\n",
    "When generating plots, try to think about what we would expect a trait distribution to look like. Should it be uniform? skewed? normal? What about the distribution of two traits? What would this look like if the traits are correlated? uncorrelated? What axes of variation might confound a clear conclusion about a trait?  \n",
    "\n",
    "We've included a number of plotting functions below to make this as easy as possible. Feel free to modify - or write your own functions - as you explore the data. \n",
    "\n",
    "\n",
    "## Goals of this section  <a class=\"tocSkip\">\n",
    "    \n",
    "1. Visualize the distribution of phenotype values  \n",
    "    - Within each continuous trait (using the kdplot function)  \n",
    "    - Within each continuous trait, organized by dichotomous data (ex: the distribution of BMI in each ancestry group - using the boxPlot function)  \n",
    "    - Between two continuous traits (with the bivariateDistributionPlot function)\n",
    "2. Determine whether trait distributions follow patterns we might expect\n",
    "3. Choose an outcome and covariates to model in the second part of this workshop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for plotting functions <a class=\"tocSkip\">\n",
    "    \n",
    "The next code block defines the plotting functions. Because running the plotting functions doesn't require understanding the syntax of the code, we have collapsed this block. Feel free to uncollapse (click the purple arrow at the right) if you're interested in all the details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define functions to easily plot phenotypes\n",
    "plt.rcParams[\"figure.figsize\"] = [6,4]\n",
    "    \n",
    "# Visualize distribution with each continuous trait\n",
    "def kdPlot(data, var):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"poster\", \n",
    "                    font_scale = 0.9, \n",
    "                    rc={\"grid.linewidth\": 0.6, 'lines.linewidth': 1.6})\n",
    "    sns.distplot(data[(var)])\n",
    "    \n",
    "# Visualize the distribution between two continuous traits\n",
    "def bivariateDistributionPlot(data, var1, var2, kind = \"scatter\"):\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        jplot = sns.jointplot(x = data[var1], y = data[var2], kind=kind, color=\"k\", s=1)\n",
    "        jplot.set_axis_labels(var1, var2)\n",
    "        \n",
    "# Visualize within each continuous trait, organized by dichotomous data\n",
    "def boxPlot(data, catagorical_var, continuous_var, color_by = None, force_x = False, force_color = False):\n",
    "    make_plot = True\n",
    "    if len(data[catagorical_var].unique().tolist()) > 10 and force_x is not True:\n",
    "        make_plot = False\n",
    "        print(\"catagorical_var must be catagorical. If you insist on using these x values, set force_x = True.\")\n",
    "    if color_by is not None:\n",
    "        if len(data[color_by].unique()) > 5 and force_color is not True:\n",
    "            make_plot = False\n",
    "            print(\"color_by column must be catagorical. If you insist on using these values, set force_color = True.\")\n",
    "    \n",
    "    if (make_plot is True):\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.set_context(\"poster\", \n",
    "                        font_scale = 0.7, \n",
    "                        rc={\"grid.linewidth\": 0.6, 'lines.linewidth': 1.6})\n",
    "        sns.boxplot(x = catagorical_var, \n",
    "                    y = continuous_var, \n",
    "                    hue = color_by, \n",
    "                    data = data, \n",
    "                    palette = [\"#275F9A\", \"#A2C353\"],\n",
    "                    saturation = 1)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating distribution plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tmajaria/ashg_2019_workshop/master/ldl_kdplot.png\" align=\"left\" width=\"20%\">\n",
    "\n",
    "***Univariate distributions*** are easily visualized in histograms or density plots. We provide a function (<font color='red'>kdplot</font>) that will generate both types of plots, overlayed in a single figure. A continuously-valued variable corresponding to a column in the phenotype dataframe should be used as input, *ldl* in this example. The function is called with the following syntax:\n",
    "\n",
    "```python\n",
    "kdPlot(samples, var = \"ldl\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tmajaria/ashg_2019_workshop/master/whr_hdl_bivariateDistributionPlot.png\" align=\"left\" width=\"20%\"> \n",
    "\n",
    "***Bivariate distributions*** can be visualized using a scatterplot. Use the function <font color='red'>bivariateDistributionPlot</font> to visualize two continuously values variables. The *type* argument determines the type of plot generated and can be one of: \"scatter\", \"reg\", \"resid\", \"kde\", and \"hex\".\n",
    "\n",
    "```python\n",
    "bivariateDistributionPlot(samples, var1 = \"hdl\", var2 = \"whr\", kind = \"scatter\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/tmajaria/ashg_2019_workshop/master/height_ancestry_boxPlot.png\" align=\"left\" width=\"20%\">\n",
    "\n",
    "***Boxplots*** can be used to further interogate relationships between continuous and categorical variables, using the <font color='red'>boxPlot</font> function. Distributions can be further subset by sex (by setting *color_by* = \"sex\").\n",
    "\n",
    "```python\n",
    "boxPlot(samples, catagorical_var = \"ancestry\", continuous_var = \"height\", color_by = \"sex\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Univariate distributions\n",
    "\n",
    "Use the code cells below to plot the distribution of single variables of your choice (such as ldl or bmi). You may need to refer to section 3.2 above for the list of variables and to section 4.1 for the plotting syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdPlot(samples, var = \"ldl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Bivariate distributions\n",
    "\n",
    "Generate scatter plots with different combinations of variables. Think about what you would expect versus what you see in the plot. You may need to refer to 3.2 for the list of variables and to section 4.1 for the plotting syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariateDistributionPlot(samples, var1 = \"hdl\", var2 = \"whr\", kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Boxplots \n",
    "\n",
    "Boxplots show relationships between continuous and categorical variables. Use the code cell below to generate a boxplot showing the relationship between height and ancestry. Try adding sex as another delineator with the `color_by` argument. You may need to refer to 3.2 for the list of variables and to section 4.1 for the plotting syntax.\n",
    "\n",
    "In the next code block, try a boxplot with variables of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxPlot(samples, catagorical_var = \"ancestry\", continuous_var = \"height\", color_by = \"sex\", force_x = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with genotype data using Hail\n",
    "----\n",
    "\n",
    "Now that we have a handle on the phenotype data, we can begin to work with the genotype data. We will make use of the [Hail](hail.is) software, an \"open-source, general-purpose, Python-based data analysis tool with additional data types and methods for working with genomic data.\" Hail utilizes distributed computing with Apache Spark for efficient genome-wide analysis.  \n",
    "\n",
    "## Goals of this section  <a class=\"tocSkip\">\n",
    "1. Load genotype data from variant call format (VCF) files into the compute environment  \n",
    "2. Understand how to access genotype and variant information  \n",
    "3. Generate variant quality control metrics for the 1000 Genomes data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short Hail primer<a class=\"tocSkip\">\n",
    "(Click the arrow at left to expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hail uses distributed computing -- a way of parallelizing tasks that can greatly decrease the **real** time it takes to complete an analysis. While distributed computing decreases **real** time, the time that you or I experience, it does not decrease **computational** time, the product of *# machines* and *time each machine was running*. Distributed computing relies on an operation called *partitioning*, dividing a object into many, many pieces. The number of partitions determines, in part, the number of operations that can run in parallel. In the code below, you will see syntax like `min_partitions = 200`. In English, this is saying \"at the minimum, divide my data into 200 pieces\". This parameter can vary and should be determined based on the data and the particular operations that you want to do -- more partitions is not always better.\n",
    "\n",
    "It is important to note that Hail expressions are **lazy** -- they are not evalutated until absolutely needed. For example, using Python for the expression `x = 2+2` would immediately store the value of `x` as `4`. However, calling the same expression using Hail would instead store `x` as `2+2` until the variable `x` was explicitly used. When using some of the code below, you will notice that some code blocks will run astoundingly quickly. This does not mean that the operation is complete, rather that the operation's result was not yet needed. When a results is needed, the operation will run. This should become more clear when working through this notebook.  \n",
    "\n",
    "For more information, checkout the [Hail documentation](http://www.nealelab.is/tools-and-software) and this [helpful video](https://youtu.be/0RTgBYL5x_E).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query workspace storage for VCF files  \n",
    "\n",
    "To find the correct VCF files for the analysis, use [Gsutil](https://cloud.google.com/storage/docs/gsutil), a command line program for accessing data from Google Cloud Storage, directly within a notebook. The <font color='red'>!</font> character has special meaning within this Jupyter notebook. It can be used to call command line functions directly and often is referred to as a [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html).\n",
    "\n",
    "For this notebook, we've hard-coded the VCF paths. **If you wanted to use different data, you would need to change this**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_base = \"gs://terra-featured-workspaces/GWAS/1kg-genotypes/subset/*.vcf.bgz\"\n",
    "\n",
    "# Use gsutil to assign the list of the files to a variable\n",
    "vcf_paths = ! gsutil ls {vcf_base}\n",
    "\n",
    "# Print a few of the paths to verify \n",
    "pprint(vcf_paths[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import packages and start a Hail session\n",
    "\n",
    "* **Hail** - an open-source, general-purpose, Python-based data analysis tool with additional data types and methods for working with genomic data  \n",
    "* **Bokeh** - an interactive visualization library  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Import some packages we will use, and set some parameters so plots render nicely\n",
    "import hail as hl\n",
    "import bokeh.io\n",
    "from bokeh.io import *\n",
    "from bokeh.resources import INLINE\n",
    "bokeh.io.output_notebook(INLINE) \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# After importing, start a Hail session\n",
    "hl.init(default_reference = \"GRCh37\", log = 'ASHG_Terra_workshop.log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VCF data and perform variant QC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 1000 Genomes data  \n",
    "\n",
    "To load genotype data from VCF files, use the <font color='red'>import_vcf</font> function. This will convert the VCF files into a **matrix table** (**mt**). A matrix table is composed of 3 parts: sample annotations (columns), variant annotations (rows), and entries (genotypes). They are optimized to allow for fast access and computation by storing small pieces of each file independently. Use the following syntax to load the 1000 Genomes data from the variable `vcf_paths` we created earlier. `vcf_paths` lists the links to all the genotype files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.import_vcf(vcf_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View matrix table structure  \n",
    "Use the <font color='red'>describe</font> function to view the structure of the matrix table:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count rows and columns**: How many variants and samples are there in your matrix table? Note that the `count` function can take a few minutes. It's a big dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge phenotype and VCF data\n",
    "\n",
    "You can merge phenotypes with the VCF data by matching sample IDs between objects. Recall that the *sample* column of the phenotype data held unique IDs for each sample. These same sample IDs are stored in the VCF files and index the matrix table columns. <font color='red'>annotate_cols</font> can be used to add the phenotypes to the matrix table columns. `samples[mt.s]` matches samples IDs between objects and <font color='red'>annotate_cols</font> merges into the VCF:  \n",
    "\n",
    "First convert the phenotypes to a Hail table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = hl.Table.from_pandas(samples, key = 'sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then annotate the matrix table by matching the sample IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_cols(pheno = samples[mt.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first few rows of the matrix table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.cols().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate variant level summary statistics \n",
    "\n",
    "To generate variant level summary statistics, use <font color='red'>variant_qc</font>. This will compute useful metrics like allele frequencies, call rate, and homozygote counts, among many others. Run variant_qc and take a look at how the matrix table structure changes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.variant_qc(mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then take a look at how the matrix table structure changes: use <fibt color=\"red\">describe</font> and you should see a new set of annotations added to the table (uner variant_qc). Then use `mt.rows().show(5)` to see the first few variants and their annotations (scroll down to the end). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.rows().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding population structure within our sample\n",
    "----\n",
    "\n",
    "Many of our statistical tests are built on the assumption that data points are unrelated and require adjustment to account for population structure. There are various ways to quantify population structure, but most start by generating a set of markers (variants) that are nearly independent of one another. For this, we will use an operation called *Linkage Disequalibrium pruning* to extract a set of variants that we can use for calculating relatedness. See [this resource](https://en.wikipedia.org/wiki/Linkage_disequilibrium) for more information on LD.\n",
    "\n",
    "Next, we will use principal component analysis (PCA) to transform the genetic data into a space that will aid in modeling by allowing us to more easily visualize genetic distance between individuals. \n",
    "\n",
    "## Goals of this section  <a class=\"tocSkip\">\n",
    "1. Generate a list of variants for calculated relatedness by filtering and LD-pruning  \n",
    "2. Calculate principal components using Hail  \n",
    "3. Visualize individuals within PC space  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant filtering\n",
    "\n",
    "To visualize population structure within our sample, we'll first generate a set of common, independent (relative to linkage disequalibrium) variants. This will ensure good representation across samples, and eliminate redundant information that may bias our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Filter variants\n",
    "First, to find where allele frequency is stored, use the <font color=red>describe</font> function again. (<font color='blue'>Hint:</font> There are two values for allele frequency, one each for the reference and alternate alleles. In our biallelic dataset, the reference frequency is the first value and the alternate frequency second.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this output, we see a nested structure called `variant_qc`. The field we are interested in is `AF`, which stands for allele frequency. `AF`'s type is an array. This array contains reference allele frequencies (at index 0) and alternate allele frequencies (at index 1).\n",
    "\n",
    "Variants can be filtered using <font color='red'>filter_rows</font> and conditioning on allele frequency. <font color='red'>filter_rows</font> takes an expression. We want to filter to variants with alternate frequency > 5% using the expression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check filtering results**: To take a look at how many variants remain in your dataset after filtering, use the <font color=\"red\">count</font> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LD-pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Run LD pruning\n",
    "\n",
    "We'll want to only include variants that are (nearly) independent of each other. We'll accomplish this using linkage disequalibrium pruning with the <font color='red'>ld_prune</font> function. Inputs are the genotypes, an r<sup>2</sup> threshold, and a window size. The r<sup>2</sup> threshold and window size control how strict we are in our definition of independence. The final parameter, *block_size*, relates to parallelization and should not be changed. More information can be found in the  [Hail documentation](https://www.hail.is).\n",
    "\n",
    "Note that this command takes some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_variants = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 100000, block_size = 1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check pruning**: Take a look at how many variants LD pruning retains with the <font color=\"red\">count</font> function. (<font color=\"blue\">Hint</font>: <font color=\"red\">describe</font> the table first so you know what the count is counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_variants.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_variants.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a count of how many variants we are pruning down to, the amount we are retaining is 25409."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Filter the matrix table \n",
    "\n",
    "The last step is to filter the matrix table again by the pruned variants list. For this, <font color='red'>is_defined</font> is useful:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.filter_rows(hl.is_defined(pruned_variants[mt.row_key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check**: \n",
    "How many variants are left after pruning? Use the count function to see the effect of pruning on the size of the matrix table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "In this next section, we'll cover a method for easily visualizing and adjusting for population structure in an association analysis: Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Run PCA\n",
    "\n",
    "You run PCA using the function <font color='red'>hwe_normalized_pca</font>. For this analysis, we are mainly interested in the scores, and can disregard the eigenvalues and loadings. The `k` parameter determines the number of PCs to return -- as `k` grows, so does the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, pcs, _ = hl.hwe_normalized_pca(mt.GT, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add PCA values to matrix table\n",
    "The PCs can then be added to the matrix table in much the same way as phenotypes. Use <font color='red'>annotate_cols</font>. (<font color='blue'>Hint:</font> the pcs object has multiple fields, use <font color='red'>describe</font> to find the field that you want to keep. Don't forget to match over the sample IDs, which is the row field `s`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use describe to find the pcs fields you want to keep \n",
    "pcs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to the matrix table\n",
    "mt = mt.annotate_cols(scores = pcs[mt.s].scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Visualize samples in PCA space\n",
    "\n",
    "Finally, let's visualize the results. Hail has some efficient built-in plotting functions -- let's use <font color='red'>plot.scatter</font>. Choose which PCs you'd like to plot and a categorical phenotype value to color the points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.scatter(mt.scores[0],\n",
    "                    mt.scores[1],\n",
    "                    label = mt.pheno.ancestry,\n",
    "                    title = 'PCA', xlabel = 'PC1', ylabel = 'PC2')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save sample metadata and update the data table\n",
    "----\n",
    "\n",
    "\n",
    "Now that you've explored the phenotypes and a measure of population stratification, the next step is to save the results and push them back to the workspace data table. Once the metadata is in the data table, you can use the data for downstream analyses.\n",
    "\n",
    "## Goals of this section  <a class=\"tocSkip\">\n",
    "1. Create a final phenotype file that includes only the outcomes and covariates desired for modeling  \n",
    "2. Generate a sample set for the workspace data table  \n",
    "3. Push all results generated in this notebook back to the workspace storage bucket, and update the Terra data table  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the phenotype data to the correct format\n",
    "\n",
    "Now that the hard computation work is done, we need to choose the data you'd like to keep, and export to a format that can be read downstream. Earlier in this notebook, you should have been thinking about which outcome and covariates you would like to model. You'll use these in a call to <font color='red'>select</font> to generate a single phenotype file with only the data you want.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sample metadata\n",
    "First, extract the sample metadata from the matrix table using <font color='red'>cols</font>:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mt.cols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data format\n",
    "The *samples* table generated in the last bit of code needs a little post processing. These steps are just to get the data into a format that is easier to work with, and their explanation is beyond the scope of this workshop. \n",
    "\n",
    "**Note:** If you saved the phenotypes or PCA scores to different variable names, you will need to adjust the code below. <font color='red'>describe</font> may help in finding the right column names.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.key_by().select('s', 'pheno', 'scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten the PC array\n",
    "If you would like to use PCs in downstream modeling, you will need to *flatten* the PC array (<font color='red'>describe</font> the table and see the type of the *scores* attribute). You can assign data to new columns in the table by using <font color='red'>annotate</font>:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.annotate(PC1 = samples.scores[0], PC2 = samples.scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the phenotype table to a Pandas data frame\n",
    "\n",
    "Hail tables are a bit difficult to write out to a regular format. You'll also want to drop the original PC scores as you made new columns for them above. It is much easier using Pandas. Convert the phenotypes into a data frame using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.drop('scores').to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data and check results\n",
    "Finally, convert the data to local dataframe and checkout the results in much the same way as with the GRM above. Write the data out and push it back to the workspace storage. (<font color='blue'>Hint:</font> The column names in the resulting data frame will reflect the nested structure of the table.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "col_map = {'s': 'sample_id', \n",
    "           'pheno.age': 'age', \n",
    "           'pheno.ancestry': 'ancestry',\n",
    "           'pheno.bmi': 'bmi',\n",
    "           'pheno.fg': 'fg',\n",
    "           'pheno.fi': 'fi',\n",
    "           'pheno.hdl': 'hdl',\n",
    "           'pheno.height': 'height',\n",
    "           'pheno.ldl': 'ldl',\n",
    "           'pheno.population': 'population',\n",
    "           'pheno.sex': 'sex',\n",
    "           'pheno.tc': 'tc',\n",
    "           'pheno.tg': 'tg',\n",
    "           'pheno.whr': 'whr'}\n",
    "samples.rename(columns = col_map, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the phenotypes out to a file and upload to the workspace storage\n",
    "\n",
    "Use *gsutil* to move the new phenotype file to the workspace storage. Don't forget to rename the \"my_phenotypes.csv\" file to be something meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_csv(\"my_ASHG_workshop_phenotypes.csv\", index = False)\n",
    "! gsutil cp my_ASHG_workshop_phenotypes.csv {bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add to the workspace data model\n",
    "\n",
    "The last step in the analysis is to update **your** workspace data table with the data generated in this notebook. For downstream analysis, you'll want to set up all the needed parameters and input data directly in the data table. As you will see, analysis workflows look to the data table for configuration and inputs.\n",
    "\n",
    "Recall that each *sample* in the data table corresponded to a single individual. To run a GWAS analysis requires a set of samples, or in Terra terms, a **sample_set**. This entity should include everything you might need to run a workflow for genotype-phenotype modeling: a list of samples to include, phenotype data, a list of covariates, an outcome, a genetic relatedness matrix, and a label. The particular workflow that you will soon run looks for inputs in specific columns of the data table. \n",
    "\n",
    "Below is a helpful function for generating a sample set in the correct format so you will have success in the next part this workshop. To look at the full code, click the arrow at the top left of the cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "def makeSampleSet(samples, billing_project, workspace, label, phenotype_file, sample_id_column = \"sample_id\", outcome = \"bmi\", covariates = None):\n",
    "    sample_set = {\n",
    "        'entity:sample_set_id': label,\n",
    "        'phenotypes': phenotype_file,\n",
    "        'outcome_name': outcome,\n",
    "        'covariates': covariates,\n",
    "        'sample_id_column': sample_id_column}\n",
    "    \n",
    "    if type(covariates) == list: sample_set['covariates'] = \",\".join(covariates)\n",
    "    elif covariates is not None: sample_set['covariates'] = covariates.replace(\" \", \",\")\n",
    "    else: del sample_set['covariates']  \n",
    "    \n",
    "    entity = '\\n'.join(['\\t'.join(sample_set.keys()),'\\t'.join(sample_set.values())])\n",
    "    fiss.fapi.upload_entities(billing_project, workspace, entity)\n",
    "\n",
    "    membership = 'membership:sample_set_id\\tsample\\n'\n",
    "    for i in range(0, samples.shape[0]):\n",
    "        membership += label + '\\t' + samples.iloc[i,0] + '\\n'\n",
    "    fiss.fapi.upload_entities(billing_project, workspace, membership) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a sample set and update the data table\n",
    "\n",
    "To use the <font color='red'>makeSampleSet</font> function, you will first need to gather a few things:  \n",
    "\n",
    "1. a label, or some short discriptor, for the sample set\n",
    "2. the phenotype filepath in the workspace storage (this should start with *gs://*)  \n",
    "3. the column name in the phenotype file that corresponds to the sample IDs (if you followed the hint above, this should be *sample_id*)  \n",
    "4. the outcome you would like to model (this must be a column name in the phenotype file)  \n",
    "5. a list of covariates to include in the model (these should be column names in the phenotype file separated by commons)  \n",
    "\n",
    "You can <font color='red'>makeSampleSet</font> with the following syntax. If you'd like to track different covariates and outcomes, you can change those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"my_ASHG_Terra_analysis\"\n",
    "phenotype_file = bucket + \"my_ASHG_workshop_phenotypes.csv\"\n",
    "sample_id_column = \"sample_id\"\n",
    "outcome = \"bmi\"\n",
    "covariates = \"age,sex,ancestry\"\n",
    "makeSampleSet(samples, billing_project, workspace, label, phenotype_file, sample_id_column, outcome, covariates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "Congratulations on completing the first steps in your GWAS analysis! You've loaded and explored the phenotype data to understand how the data are distributed, loaded the VCF data and performed variant QC, filtered variants and done LD-pruning to filer the matrix table, run a PCA, and added PCA values to the matrix table. You've saved the sample data and updated it to your workspace data table. \n",
    "\n",
    "The next step is to use the data to do a mixed-model association test in a workflow. Save and close this notebook and proceed to the dashboard for instructions of how to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations:  \n",
    "\n",
    "1. A global reference for human genetic variation, The 1000 Genomes Project Consortium, Nature 526, 68-74 (01 October 2015) doi:10.1038/nature15393  \n",
    "2. Yengo L, Sidorenko J, Kemper KE, Zheng Z, Wood AR, Weedon MN, Frayling TM, Hirschhorn J, Yang J, Visscher PM, GIANT Consortium. (2018). Meta-analysis of genome-wide association studies for height and body mass index in ~700,000 individuals of European ancestry. Biorxiv  \n",
    "3. Scott RA et al. Large-scale association analyses identify new loci influencing glycemic traits and provide insight into the underlying biological pathways. Nat. Genet. 2012;44;9;991-1005  \n",
    "4. Klarin D, et al. Genetics of blood lipids among ~300,000 multi-ethnic participants of the Million Veteran Program. Nat. Genet. 2018;50:1514–1523. doi: 10.1038/s41588-018-0222-9.  \n",
    "5. Hail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.13.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise hints <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4  <a class=\"tocSkip\">\n",
    "```python\n",
    "4.1.1 kdPlot(samples, var = \"ldl\")\n",
    "4.1.2 bivariateDistributionPlot(samples, var1 = \"hdl\", var2 = \"whr\", kind = \"scatter\")\n",
    "4.1.3 boxPlot(samples, catagorical_var = \"ancestry\", continuous_var = \"height\", color_by = \"sex\")\n",
    "```\n",
    "    \n",
    "### Section 5  <a class=\"tocSkip\">\n",
    "```python\n",
    "5.3.1 mt = hl.import_vcf(vcf_paths)\n",
    "5.3.2 mt.describe()\n",
    "      mt.count()\n",
    "5.3.3 samples = hl.Table.from_pandas(samples, key = 'sample')\n",
    "      mt = mt.annotate_cols(pheno = samples[mt.s])\n",
    "      mt.rows().show(5)\n",
    "5.3.4 mt = hl.variant_qc(mt)\n",
    "      mt.describe()\n",
    "      mt.rows().show(5)\n",
    "```\n",
    "\n",
    "### Section 6  <a class=\"tocSkip\">\n",
    "```python\n",
    "6.1.1 mt.describe()\n",
    "      mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.05)\n",
    "      mt.count()\n",
    "    \n",
    "6.2.1 pruned_variants = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 100000, block_size = 1024)\n",
    "      pruned_variants.describe()\n",
    "      pruned_variants.count()\n",
    "6.2.2 mt = mt.filter_rows(hl.is_defined(pruned_variants[mt.row_key]))\n",
    "      mt.count()\n",
    "\n",
    "6.3.1 _, pcs, _ = hl.hwe_normalized_pca(mt.GT, k=5) \n",
    "6.3.2 pcs.describe()\n",
    "      mt = mt.annotate_cols(scores = pcs[mt.s].scores)    \n",
    "6.3.3 p = hl.plot.scatter(mt.scores[0],   \n",
    "                    mt.scores[1],    \n",
    "                    label = mt.pheno.ancestry,    \n",
    "                    title = 'PCA', xlabel = 'PC1', ylabel = 'PC2')    \n",
    "    show(p)    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258.641px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
